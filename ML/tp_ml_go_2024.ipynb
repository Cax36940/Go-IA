{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzTfCbHzN1Wj"
   },
   "source": [
    "# Apprenez une heuristique pour le Go\n",
    "\n",
    "Dans ce **TP noté**, vous devrez déployer des methodes d'apprentissage automatique permettant d'évaluer la qualité de plateaux de GO.\n",
    "\n",
    "Pour cela, vous disposerez de 21854 exemples de plateau de Go, tous générés par `gnugo` après quelques coups contre lui même avec un niveau de difficulté de 0. Par chaque plateau, nous avons lancé 100 matchs de gnugo contre lui même, toujours avec un niveau 0, et compté le nombre de victoires de noir et de blanc depuis ce plateau.\n",
    "\n",
    "A noter, chaque \"rollout\" (un rollout et un déroulement possible du match depuis le plateau de référence) correspond à des mouvements choisis aléatoirement parmis les 10 meilleurs mouvements possibles, en biasant le choix aléatoire par la qualité prédite du mouvement par gnugo (les meilleurs mouvements ont une plus forte probabilité d'être tirés).\n",
    "\n",
    "Les données dont vous disposez sont brutes. Ce sera à vous de proposer un format adéquat pour utiliser ces données en entrée de votre réseau neuronal.\n",
    "\n",
    "\n",
    "## Comment sera évalué votre modèle ?\n",
    "\n",
    "Nous vous fournirons 6h avant la date de rendu un nouveau fichier contenant 1000 nouveaux exemples, qui ne contiendront pas les champs `black_wins`, `white_wins`, `black_points` et `white_points`. Vous devrez laisser, dans votre dépot de projet (votre dépot GIT sous un sous-répertoire ML) un fichier texte nommé `my_predictions.txt` ayant une prédiction par ligne (un simple flottant) qui donnera, dans le même ordre de la liste des exemples les scores que vous prédisez pour chacune des entrées du fichier que nous vous aurons donné. ** Les scores seront donnés sous forme d'un flottant, entre 0 et 1, donnant la probabilité de victoire de noir sur le plateau considéré **. Il faudra laisser, dans votre feuille notebook (voir tout en dessous) la cellule Python qui aura créé ce fichier, pour que l'on puisse éventuellement refaire vos prédictions.\n",
    "\n",
    "Bien entendu, vous nous rendrez également votre feuille jupyter **sous deux formats**, à la fois le fichier `.ipynb` et le fichier `.html` nous permettant de lire ce que vous avez fait, sans forcément relancer la feuille. Nous prendrons en compte les résultats obtenus sur les prédictions mais aussi le contenu de vos notebooks jupyter.\n",
    "\n",
    "### Comment sera noté ce TP ?\n",
    "\n",
    "**Il s'agit d'un TP noté (et non pas d'un projet), donc il ne faudra pas y passer trop de temps**. Nous attendons des prédictions correctes mais surtout des choix justifiés dans la feuille. Votre feuille notebook sera le plus important pour la notation (n'hésitez pas à mettre des cellules de texte pour expliquer pourquoi vous avez été amenés à faire certains choix). Ainsi, il serait bien d'avoir, par exemple, les données (graphiques ou autre) qui permettent de comprendre comment vous avez évité l'overfitting.\n",
    "\n",
    "Le fichier de vos prédiction sera évalué en se basant sur la qualité de vos prédictions. Nous mesurerons par exemple (juste pour vous donner une idée) le nombre d'exemples dont votre prédiction donnera la bonne valeur à 5%, 10%, 20%, 35%, 50% pour estimer sa qualité.\n",
    "\n",
    "\n",
    "## Mise en route !\n",
    "\n",
    "Voyons  comment lire les données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQyK1qLZN1Wl",
    "outputId": "5924f94d-5257-4361-8e84-e1f7dc050685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 664848 examples\n",
      "white: 355600 black: 309248\n",
      "white: 355600 black: 355600\n"
     ]
    }
   ],
   "source": [
    "# Import du fichier d'exemples\n",
    "import numpy as np\n",
    "\n",
    "def get_processed_data_go():\n",
    "\timport json\n",
    "\twith open('database-8x8.json', 'r') as f:\n",
    "\t\treturn json.load(f)\n",
    "\n",
    "data = get_processed_data_go()\n",
    "print(\"We have\", len(data),\"examples\")\n",
    "\n",
    "def prepare_data(data):\n",
    "\tsizetraining = int(len(data) * 0.33)\n",
    "\tblack = []\n",
    "\twhite = []\n",
    "\tfor games in data:\n",
    "\t\tif games[\"black_wins\"]/100 >= 0.5:\n",
    "\t\t\tblack.append(games)\n",
    "\t\telse:\n",
    "\t\t\twhite.append(games)\n",
    "\tnbblack = len(black)\n",
    "\tnbwhite = len(white)\n",
    "\n",
    "\tprint(\"white:\", nbwhite, \"black:\", nbblack)\n",
    "\n",
    "\twhile nbblack > nbwhite:\n",
    "\t\tshuffledwhite = white.copy()\n",
    "\t\twhite = np.concatenate((white, shuffledwhite[:nbblack - nbwhite]))\n",
    "\t\tnbwhite = len(white)\n",
    "\t\tnp.random.shuffle(white)\n",
    "\n",
    "\twhile nbwhite > nbblack:\n",
    "\t\tshuffledblack = black.copy()\n",
    "\t\tblack = np.concatenate((black, shuffledblack[:nbwhite - nbblack]))\n",
    "\t\tnbblack = len(black)\n",
    "\t\tnp.random.shuffle(black)\n",
    "\n",
    "\tprint(\"white:\", nbwhite, \"black:\", nbblack)\n",
    "\n",
    "\ttrain_white = white[:sizetraining]\n",
    "\tval_white = white[sizetraining:]\n",
    "\ttrain_black = black[:sizetraining]\n",
    "\tval_black = black[sizetraining:]\n",
    "\ttrain_data = np.concatenate((train_black, train_white))\n",
    "\tnp.random.shuffle(train_data)\n",
    "\tval_data = np.concatenate((val_black, val_white))\n",
    "\tnp.random.shuffle(val_data)\n",
    "\treturn train_data, val_data\n",
    "\n",
    "size = 30000\n",
    "new_datas = prepare_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaSjA3uPN1Wn"
   },
   "source": [
    "## Compréhension des données de chaque entrée\n",
    "\n",
    "Voici une description de chaque exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCKzso-aN1Wn",
    "outputId": "96be7d06-ba1a-4c17-9ae2-2970695125f5"
   },
   "outputs": [],
   "source": [
    "def summary_of_example(data, sample_nb):\n",
    "    ''' Gives you some insights about a sample number'''\n",
    "    sample = data[sample_nb]\n",
    "    print(\"Sample\", sample_nb)\n",
    "    print()\n",
    "    print(\"Données brutes en format JSON:\", sample)\n",
    "    print()\n",
    "    print(\"The sample was obtained after\", sample[\"depth\"], \"moves\")\n",
    "    print(\"The successive moves were\", sample[\"list_of_moves\"])\n",
    "    print(\"After these moves and all the captures, there was black stones at the following position\", sample[\"black_stones\"])\n",
    "    print(\"After these moves and all the captures, there was white stones at the following position\", sample[\"white_stones\"])\n",
    "    print(\"Number of rollouts (gnugo games played against itself from this position):\", sample[\"rollouts\"])\n",
    "    print(\"Over these\", sample[\"rollouts\"], \"games, black won\", sample[\"black_wins\"], \"times with\", sample[\"black_points\"], \"total points over all this winning games\")\n",
    "    print(\"Over these\", sample[\"rollouts\"], \"games, white won\", sample[\"white_wins\"], \"times with\", sample[\"white_points\"], \"total points over all this winning games\")\n",
    "\n",
    "# summary_of_example(data,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-AdNZZEN1Wo"
   },
   "source": [
    "## Données en entrée et en sortie de votre modèle final\n",
    "\n",
    "Même si en interne, votre modèle va manipuler des tenseurs en numpy, vous devrez construire une boite noire qui prendra en entrée les données dans le style du JSON ci-dessous. Typiquement, vous aurez le même genre de fichier avec seulement les champs `black_stones`, `white_stones`, `depth` et `list_of_moves` de renseignées. Vous devrez utiliser ces champs, dont notemment les coordonnées des pierres noires et blanches et donner le pourcentage de chance pour noir de gagner depuis cette position.\n",
    "\n",
    "Ainsi, pour l'exemple `i` :\n",
    "- Vous pourrez prendez en entree `data[i][\"black_stones\"]` et `data[i][\"white_stones\"]` (vous pouvez, si vous le souhaitez, prendre en compte également `list_of_moves` ou tout autre donnée que vous calculerez vous-même (mais qui ne se basera évidemment pas sur les données que vous n'aurez pas lors de l'évaluation).\n",
    "- Vous devrez prédire simplement `data[i][\"black_wins\"]/data[i][\"rollouts\"]` en float (donc une valeur entre 0 et 1).\n",
    "\n",
    "Encore une fois, **attention** : en interne, il faudra absolument construire vos données formattées en matrices numpy pour faire votre entrainement. On vous demande juste ici d'écrire comment vous faites ces transformations, pour comprendre ce que vous avez décidé de mettre en entrée du réseau.\n",
    "\n",
    "Voici par exemple le modèle de la fonction qui pourra être appelée, au final :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxeEQ89HN1Wp",
    "outputId": "9956cdb6-ec15-4085-b917-bf89c169bf3b"
   },
   "outputs": [],
   "source": [
    "def position_predict(black_stones, white_stones):\n",
    "\n",
    "    # ... Votre tambouille interne pour placer les pierres comme il faut dans votre structure de données\n",
    "    # et appeler votre modèle Keras (typiquement avec model.predict())\n",
    "    prediction = None # model.predict(...) # A REMPLIR CORRECTEMENT\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Par exemple, nous pourrons appeler votre prédiction ainsi\n",
    "\n",
    "# print(\"Prediction this sample:\")\n",
    "# data_index = 1\n",
    "# summary_of_example(data, data_index)\n",
    "# print()\n",
    "# prediction = position_predict(data[data_index][\"black_stones\"], data[data_index][\"white_stones\"])\n",
    "# print(\"You predicted\", prediction, \"and the actual target was\", data[data_index][\"black_wins\"]/data[data_index][\"rollouts\"])\n",
    "\n",
    "# Ainsi, pour le rendu, en admettant que newdata soit la structure de données issue du json contenant les nouvelles données que\n",
    "# l'on vous donnera 24h avant la fin, vous pourrez construire le fichier resultat ainsi\n",
    "\n",
    "def create_result_file(newdata):\n",
    "    ''' Exemple de méthode permettant de générer le fichier de resultats demandés. '''\n",
    "    resultat  = [position_predict(d[\"black_stones\"], d[\"white_stones\"]) for d in newdata]\n",
    "    with open(\"my_predictions.txt\", \"w\") as f:\n",
    "         for p in resultat:\n",
    "            f.write(str(p)+\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944
    },
    "id": "sIGMJhBUN1Wq",
    "outputId": "5417c7ef-dbca-4891-ed17-e3d2c6d74968"
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.title(\"Relationship between the depth of the board and the chance for black to win\")\n",
    "# plt.plot([sample[\"black_wins\"] for sample in data],[sample[\"depth\"] for sample in data], '.')\n",
    "# plt.xlabel(\"black wins (percentage)\")\n",
    "# plt.ylabel(\"depth of the game\")\n",
    "\n",
    "\n",
    "# # Cumulative Distribution function of the chance of black to win\n",
    "# cdf_wins = sorted([sample[\"black_wins\"] for sample in data])\n",
    "# plt.figure()\n",
    "# plt.plot([x/len(cdf_wins) for x in range(len(cdf_wins))], cdf_wins)\n",
    "# plt.grid()\n",
    "# plt.title(\"Cumulative Distribution function of the chance of black to win\")\n",
    "# plt.xlabel(\"% of the samples with a chance of black to win below the y value\")\n",
    "# plt.ylabel(\"Chance of black to win\")\n",
    "# print(\"The CDF curve shows that black has more chances to win, globally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJn7gQooN1Wq"
   },
   "source": [
    "# First steps: transform all the data into numpy arrays to feed your neural network\n",
    "\n",
    "Advices:\n",
    "- do not use only a 9x9 matrix as input. Use at least two planes to encode the board. One plane for black and one plane for white (typically with a 1 if there is a black stone for the first plane and with a 1 if there is a white stone for the second plane). The dimension of an input should be at least `[2,9,9]`. In Torch, the Conv2d method needs inputs as `[NBatch, Channels, H, W]`.\n",
    "- consider to enrich your dataset with all symmetries and rotations. You should be able to multiply the number of samples to consider: any rotation of the board should have the same score, right?. You can use `np.rot90` to rotate your boards be beware of the dimensions (the channel is not the last dimension), so you may want to use `np.moveaxis()` to force the channels to be the last dimension, then call it again to make it the second one.\n",
    "- what should happen on the score if you switch the colors? To know which player has to play next, you can check, for a sample, the parity of the length of the list `data[i][\"list_of_moves\"]` (an odd length list would mean that white is the next player. An even length list means that black has to play).\n",
    "- work on enlarging and preparing your data only once. Once all you input data is setup as a big Numpy matrix, you may want to save it for speeding up everything. You can use, for instance `numpy.rot90()` and `numpy.flipud()` to generate all the symmetries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tdT6ucKcN1Wr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 1. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert to tensor that the ML will take as input\n",
    "def coord_to_board(black_coord, white_coord):\n",
    "    board = np.zeros((2,8,8), dtype=np.double)\n",
    "    for black in black_coord:\n",
    "        board[0][(black[0],black[1])] = 1\n",
    "    for white in white_coord:\n",
    "        board[1][(white[0], white[1])] = 1\n",
    "    return np.array(board)\n",
    "\n",
    "print(coord_to_board(data[0][\"black_coord\"], data[0][\"white_coord\"]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbzFMh-UN1Wr"
   },
   "source": [
    "# Second steps: build your neural network and train it\n",
    "\n",
    "Don't forget to check overfitting, ...\n",
    "\n",
    "*advices* :\n",
    "- you may need to use some of the `torch.nn` layers: `Linear`, `Conv2d`, `ReLU`, `LeakyReLU`, `BatchNorm2d`, `Flatten`, `Dropout`... But you can of course first build a very simple one (and just pick some of them)...\n",
    "- if you use convolution layers, be sure **not to downsize your board**. Applying a filter should keep the original size of the board (9x9), otherwise you would somehow forget the stones on the borders\n",
    "- you will use like 33% of your input sample for validation. However, the final goal is to score new data that will be given in addition to the actual data. So, you should use the 33% splitting rule to set up your network architecture and, once you fixed it, you should train your final model on the whole set of data, crossing your fingers that it will generalize well.\n",
    "- Warning: if you run a few epoch, and run it again for some more epochs, it will not reset the weights and the biases of your neural network. It's good news because you can add more and more epochs to your model, but be careful about the training/test sets (do split your sets before you initialize your model). Or you will be breaking your validation/training partition!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IQ1siAgaN1Ws"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white: 528 black: 472\n",
      "white: 528 black: 528\n",
      "21\n",
      "{'loss': 0.3339519016670458} {'loss': 0.30940243619861024, 'correct %': 24.24}\n",
      "{'loss': 0.3071307636571653} {'loss': 0.3032846855394768, 'correct %': 21.21}\n",
      "{'loss': 0.2983791784806685} {'loss': 0.29591834834127717, 'correct %': 26.21}\n",
      "{'loss': 0.2921107010407881} {'loss': 0.2862176360506, 'correct %': 33.18}\n",
      "{'loss': 0.2864848427700274} {'loss': 0.2829976529786081, 'correct %': 39.09}\n",
      "{'loss': 0.28390628334247703} {'loss': 0.28359392700773295, 'correct %': 37.87}\n",
      "{'loss': 0.28249978043816304} {'loss': 0.28102580345038214, 'correct %': 44.84}\n",
      "{'loss': 0.28122228943940364} {'loss': 0.28069913676290803, 'correct %': 45.15}\n",
      "{'loss': 0.2807339104739102} {'loss': 0.2804088260188247, 'correct %': 48.93}\n",
      "{'loss': 0.2804478605588277} {'loss': 0.2803544008370602, 'correct %': 53.63}\n",
      "{'loss': 0.2802241092378443} {'loss': 0.2799546256209865, 'correct %': 54.54}\n",
      "{'loss': 0.28012362610210073} {'loss': 0.2798281546795007, 'correct %': 55.3}\n",
      "{'loss': 0.28015389893994186} {'loss': 0.2798235813776652, 'correct %': 57.12}\n",
      "{'loss': 0.2801194406820066} {'loss': 0.2798411390998147, 'correct %': 55.9}\n",
      "{'loss': 0.28003787696361543} {'loss': 0.2798643617918997, 'correct %': 53.93}\n",
      "{'loss': 0.27981668331406334} {'loss': 0.27956584150140934, 'correct %': 62.42}\n",
      "{'loss': 0.27988118637691844} {'loss': 0.27963872317111854, 'correct %': 57.42}\n",
      "{'loss': 0.2801536337895827} {'loss': 0.2804982734448982, 'correct %': 49.84}\n",
      "{'loss': 0.28076591311079085} {'loss': 0.2811553088101474, 'correct %': 48.33}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Conv2d, Tanh, ReLU, Dropout, Linear, MaxPool2d\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = torch.device('mps')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "def train(model, device, train_loader,  optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device).float(), target.to(device).float()\n",
    "        target = target.view(-1, 1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.binary_cross_entropy(output, target, reduction=\"mean\") # Read the documentation.\n",
    "        total_loss += loss.item() * len(data) # We average the loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "    return {\"loss\": total_loss}\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device).float(), target.to(device).float()\n",
    "            target = target.view(-1, 1)\n",
    "            output = model(data)\n",
    "            loss = F.binary_cross_entropy(output, target, reduction=\"sum\")\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.round(output * 100) / 100\n",
    "            correct += predictions.eq(target.view_as(predictions)).sum().item()\n",
    "    total_loss /= len(test_loader.dataset)\n",
    "    return {\"loss\": total_loss, \"correct %\": int(10000*correct/len(test_loader.dataset))/100}\n",
    "\n",
    "\n",
    "\n",
    "class GoFindWin(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GoFindWin, self).__init__()\n",
    "    self.flatten = nn.Flatten(start_dim=1) # Do not flatten the batch dimension\n",
    "\n",
    "    self.conv1 = Conv2d(2, 8, 3, padding=\"same\")\n",
    "    self.conv2 = Conv2d(8, 20, 3, padding=\"same\")\n",
    "    self.sub1 = MaxPool2d(2)\n",
    "    self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "    self.dense1 = nn.Linear(20*4*4, 32)\n",
    "    self.dense2 = nn.Linear(32, 1)\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.tanh(x)\n",
    "    x = self.conv2(x)\n",
    "    x = F.tanh(x)\n",
    "    x = self.sub1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense1(x)\n",
    "    x = F.tanh(x)\n",
    "    x = self.dense2(x)\n",
    "    out = F.sigmoid(x) # In torch, we can either use log_softmax + nll_loss or the CrossEntropy (without the last layer of softmax)\n",
    "    return out\n",
    "\n",
    "\n",
    "def format_data(datas):\n",
    "    datas_length = len(datas)\n",
    "    X = []\n",
    "    Y = np.zeros(datas_length, dtype=np.double)\n",
    "    for i in range(datas_length):\n",
    "        X.append(coord_to_board(datas[i][\"black_coord\"], datas[i][\"white_coord\"]))\n",
    "        Y[i] = datas[i][\"black_wins\"]/100\n",
    "    return np.array(X), Y\n",
    "\n",
    "model = torch.load(\"model.pth\")\n",
    "# model = GoFindWin().to(device).float()\n",
    "\n",
    "# print(model)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data, test_data = prepare_data(data[:1000])\n",
    "\n",
    "train_x, train_y = format_data(train_data)\n",
    "test_x, test_y = format_data(train_data)\n",
    "# print(train_data[0])\n",
    "# print(train_x[0], train_y[0])\n",
    "# print(test_x[0], test_y[0])\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(train_x, train_y)), batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "testloader = torch.utils.data.DataLoader(list(zip(test_x, test_y)), batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "print(len(testloader))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(1, 20):\n",
    "  train_stats = train(model, device, trainloader, optimizer, epoch)\n",
    "  test_stats = test(model, device, testloader)\n",
    "  print(train_stats, test_stats)\n",
    "\n",
    "torch.save(model, \"model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVnqHnBqN1Ws"
   },
   "source": [
    "# Last step\n",
    "\n",
    "Prepare your model to predict the set of new data to predict, you will have only 6 hours to push your predictions.\n",
    "\n",
    "(may be you would like to express, when guessing the percentage of wins for blacks, that it should reflect the fact that this score should be the same for all the symmetries you considered)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39RJHPbmN1Ws"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
